%% PPGCC - CCN - UFPI

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
%% Exemplo de arquivo .bib a ser utilizado para gerar a Biliografia

%% Quando for no Google Scholar, clique em bibtex na citação e copie o código pra colar aqui

@article{haenlein2019BriefHistory,
author = {Michael Haenlein and Andreas Kaplan},
title ={A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence},

journal = {California Management Review},
volume = {61},
number = {4},
pages = {5-14},
year = {2019},
doi = {10.1177/0008125619864925},

URL = { 
    
        https://doi.org/10.1177/0008125619864925
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0008125619864925
    
    

}
,
    abstract = { This introduction to this special issue discusses artificial intelligence (AI), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world’s leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives. }
}

@article{mijwil2022future,
  author    = {Maad M. Mijwil and Karan Aggarwal and Sonia Sonia and Abdel Hameed Al-Mistarehi and Safwan Alomari and Murat Gök and Anas M. Zein Alaabdin and Safaa H. Abdulrhman},
  title     = {Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning},
  journal   = {Iraqi Journal for Computer Science and Mathematics},
  volume    = {3},
  number    = {1},
  year      = {2022},
  article   = {13},
  doi       = {10.52866/ijcsm.2022.01.01.013},
  url       = {https://ijcsm.researchcommons.org/ijcsm/vol3/iss1/13}
}


@InProceedings{koh2017understang,
  title = 	 {Understanding Black-box Predictions via Influence Functions},
  author =       {Pang Wei Koh and Percy Liang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1885--1894},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/koh17a.html},
  abstract = 	 {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.}
}

@article{arrieta2020explainable,
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
journal = {Information Fusion},
volume = {58},
pages = {82-115},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}

@article{https://doi.org/10.1002/aisy.202400304,
author = {Salih, Ahmed M. and Raisi-Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Lekadir, Karim and Menegaz, Gloria},
title = {A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
journal = {Advanced Intelligent Systems},
volume = {7},
number = {1},
pages = {2400304},
keywords = {collinearity, interpretability, Local Interpretable Model Agnostic Explanation, SHapley Additive exPlanations, eXplainable artificial intelligence},
doi = {https://doi.org/10.1002/aisy.202400304},
url = {https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202400304},
eprint = {https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202400304},
abstract = {eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning (ML) models into a more digestible form. These methods help to communicate how the model works with the aim of making ML models more transparent and increasing the trust of end-users in their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods, particularly with tabular data. In this perspective piece, the way the explainability metrics of these two methods are generated is discussed and a framework for the interpretation of their outputs, highlighting their weaknesses and strengths is proposed. Specifically, their outcomes in terms of model-dependency and in the presence of collinearity among the features, relying on a case study from the biomedical domain (classification of individuals with or without myocardial infarction) are discussed. The results indicate that SHAP and LIME are highly affected by the adopted ML model and feature collinearity, raising a note of caution on their usage and interpretation.},
year = {2025}
}

@article{salih2025a_perpective,
author = {Salih, Ahmed M. and Raisi-Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Lekadir, Karim and Menegaz, Gloria},
title = {A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
journal = {Advanced Intelligent Systems},
volume = {7},
number = {1},
pages = {2400304},
keywords = {collinearity, interpretability, Local Interpretable Model Agnostic Explanation, SHapley Additive exPlanations, eXplainable artificial intelligence},
doi = {https://doi.org/10.1002/aisy.202400304},
url = {https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202400304},
eprint = {https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202400304},
abstract = {eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning (ML) models into a more digestible form. These methods help to communicate how the model works with the aim of making ML models more transparent and increasing the trust of end-users in their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods, particularly with tabular data. In this perspective piece, the way the explainability metrics of these two methods are generated is discussed and a framework for the interpretation of their outputs, highlighting their weaknesses and strengths is proposed. Specifically, their outcomes in terms of model-dependency and in the presence of collinearity among the features, relying on a case study from the biomedical domain (classification of individuals with or without myocardial infarction) are discussed. The results indicate that SHAP and LIME are highly affected by the adopted ML model and feature collinearity, raising a note of caution on their usage and interpretation.},
year = {2025}
}

@misc{Salih2024MiniReview,
      title={Explainable Artificial Intelligence and Multicollinearity : A Mini Review of Current Approaches}, 
      author={Ahmed M Salih},
      year={2024},
      eprint={2406.11524},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.11524}, 
}

@InProceedings{Basu2022Multicollinearity,
author="Basu, Indranil
and Maji, Subhadip",
editor="Long, Guodong
and Yu, Xinghuo
and Wang, Sen",
title="Multicollinearity Correction and Combined Feature Effect in Shapley Values",
booktitle="AI 2021: Advances in Artificial Intelligence",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="79--90",
abstract="Model interpretability is one of the most intriguing problems in most machine learning models, particularly for those that are mathematically sophisticated. Computing Shapley Values are one of the best approaches so far to find the importance of each feature in a model, at the instance (data point) level. In other words, Shapley values represent the importance of a feature for a particular instance or observation, especially for classification or regression problems. One of the well known limitations of Shapley values is that the estimation of Shapley values with the presence of multicollinearity among the features are not accurate as well as reliable. To address this problem, we present a unified framework to calculate accurate Shapley values with correlated features. To be more specific, we do an adjustment (matrix formulation) of the features while calculating independent Shapley values for the instances to make the features independent with each other. Our implementation of this method proves that our method is computationally efficient also, compared to the existing Shapley method.",
isbn="978-3-030-97546-3"
}

@INPROCEEDINGS{Salih2022Investigating,
  author={Salih, Ahmed and Galazzo, Ilaria Boscolo and Cruciani, Federica and Brusini, Lorenza and Radeva, Petia},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, 
  title={Investigating Explainable Artificial Intelligence for MRI-based Classification of Dementia: a New Stability Criterion for Explainable Methods}, 
  year={2022},
  volume={},
  number={},
  pages={4003-4007},
  keywords={Image processing;Stability criteria;Machine learning;Robustness;Alzheimer's disease;Monitoring;Explainability;XAI;Multicollinearity;Proxy},
  doi={10.1109/ICIP46576.2022.9897253}}

@ARTICLE{Salih2024Characterizing,
  author={Salih, Ahmed M and Galazzo, Ilaria Boscolo and Raisi-Estabragh, Zahra and Petersen, Steffen E. and Menegaz, Gloria and Radeva, Petia},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Characterizing the Contribution of Dependent Features in XAI Methods}, 
  year={2024},
  volume={28},
  number={11},
  pages={6466-6473},
  keywords={Explainable AI;Data models;Nuclear magnetic resonance;Biological system modeling;Noise measurement;Bioinformatics;Kernel;Dependency;proxy;XAI},
  doi={10.1109/JBHI.2024.3395289}}

@inproceedings{Salih2025AdditiveEffects,
author = {Salih, Ahmed M},
title = {Explainable Artificial Intelligence for Dependent Features: Additive Effects of Collinearity},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704152},
doi = {10.1145/3704137.3704152},
abstract = {Explainable Artificial Intelligence (XAI) emerged to reveal the internal mechanism of machine learning models and how the features affect the prediction outcome. Collinearity is one of the big issues that XAI methods face when identifying the most informative features in the model. Current XAI approaches assume the features in the models are independent and calculate the effect of each feature toward model prediction independently from the rest of the features. However, such assumption is not realistic in real life applications. We propose an Additive Effects of Collinearity (AEC) as a novel XAI method that aim to considers the collinearity issue when it models the effect of each feature in the model on the outcome. AEC is based on the idea of dividing multivariate models into several univariate models in order to examine their impact on each other and consequently on the outcome. The proposed method is implemented using simulated and real data to validate its efficiency comparing with the a state of arts XAI method. The results indicate that AEC is more robust and stable against the impact of collinearity when it explains AI models compared with the state of arts XAI method.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {94–99},
numpages = {6},
keywords = {XAI, Collinearity, AEC},
location = {
},
series = {ICAAI '24}
}


@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

%---------------------------------------------------------
%referencias que já existiam de exemplo