% ----------------------------------------------------------
% Introdução 
% Capítulo sem numeração, mas presente no Sumário
% ----------------------------------------------------------

\chapter[Fundamentação Teórica]{Fundamentação Teórica}




%\section{Colineariedade}\label{sec:colineariedade}
%\lipsum[35]



%\section{Inteligência Artificial Explicável}\label{sec:xai}
%\lipsum[35]

%\subsection{SHAP}
%\lipsum[35]

%\subsection{LIME}
%\lipsum[35]

%\section{Algoritmos de Classificação}\label{sec:alg_class}
%\lipsum[35]
%\section {Problemas de classificação}
%\subsection{Algoritmos utilizados em problemas de classificação}\label{sec:algoritmos_classificacao}

%\section{Inteligência Artificial Explicável (XAI)}

%\subsection{Algoritmos de XAI}\label{sec:algoritmos_xai}

\section{Trabalhos relacionados}\label{sec:trabalhos_relacionados}

Alguns trabalhos foram desenvolvidos com o objetivo de compreender e mitigar os efeitos da colinearidade nos algoritmos de Inteligência Artificial Explicável (XAI). \cite{Basu2022Multicollinearity} apresentaram um framework matemático para corrigir a multicolinearidade no cálculo de valores de Shapley. Eles argumentam que a versão tradicional do SHAP assume independência entre features, o que não é realista e pode distorcer a atribuição de importância. Para resolver isso, propuseram um ajuste matricial que corrige os valores de Shapley considerando as correlações entre as variáveis, de modo que a importância individual se torne independente da multicolinearidade. Além disso, estenderam essa correção para calcular efeitos combinados de pares (ou grupos) de variáveis correlacionadas, somando os valores ajustados. O método foi validado em problemas reais de classificação, demonstrando boa eficiência computacional e explicações mais consistentes em presença de variáveis correlacionadas.

Dando continuidade à discussão sobre os impactos da multicolinearidade na explicabilidade, \cite{Salih2022Investigating} propuseram um novo critério de estabilidade para avaliar técnicas de XAI aplicadas à classificação de demência com base em imagens de ressonância magnética. Os autores treinaram modelos de classificação para diferenciar pacientes com e sem demência e analisaram a robustez dos rankings de importância gerados por métodos explicativos. A principal contribuição foi o desenvolvimento do NMR (Normalized Movement Rate), um critério que quantifica a estabilidade das explicações fornecidas por técnicas de XAI, especialmente em cenários com multicolinearidade entre atributos. Os resultados demonstraram que o NMR melhora a confiabilidade na identificação de variáveis informativas, contribuindo para uma personalização mais segura do monitoramento clínico.

A partir da mesma motivação, \citeonline{Salih2024Characterizing} apresentaram o método Modified Index Position (MIP) como uma solução simples e agnóstica ao modelo para ajustar os rankings de importância de variáveis gerados por métodos de XAI, como o SHAP, especialmente em contextos com colinearidade entre atributos. A abordagem consiste em remover iterativamente a variável mais importante apontada pela técnica de XAI, retreinar o modelo e reaplicar a explicabilidade, observando como as demais variáveis mudam de posição no ranking. Isso permite reordenar a importância original de forma a refletir dependências entre as variáveis. Aplicado a uma tarefa de classificação de gênero (homem ou mulher) com base em nove fenótipos cardíacos, o método demonstrou rankings mais robustos e menos sensíveis à colinearidade em comparação ao SHAP tradicional, sendo validado por análise de componentes principais e plausibilidade biológica.

Complementando esse estudo, \citeonline{salih2025a_perpective} investigaram as limitações dos métodos SHAP e LIME ao aplicá-los separadamente em dois conjuntos de dados distintos. Em cada experimento, treinaram quatro modelos de classificação (LightGBM, Regressão Logística, Árvore de Decisão e SVC) e analisaram como cada variável era explicada por cada modelo. Os resultados mostraram que ambos os métodos apresentaram forte dependência do modelo, com variações na ordem e na direção da contribuição das variáveis em cada classificador. Além disso, a presença de colinearidade comprometeu a interpretação, já que variáveis correlacionadas recebiam baixa importância por serem explicadas por outras. Para mitigar esses efeitos, os autores propuseram o uso da métrica NMR para avaliar a estabilidade das explicações entre os modelos e o método MIP para ajustar a importância das variáveis considerando a multicolinearidade. 

Explorando outra abordagem para lidar com esse desafio, \citeonline{Salih2025AdditiveEffects} propôs o método Additive Effects of Collinearity (AEC) para superar as limitações de métodos de XAI em contextos com colinearidade entre variáveis. O autor argumenta que técnicas como o SHAP e LIME assumem independência entre os atributos, o que pode distorcer os rankings de importância. O AEC contorna esse problema ao decompor modelos multivariáveis em modelos univariáveis, estimando o efeito isolado de cada variável e, depois, somando esses efeitos considerando suas interdependências. O método foi validado em tarefas de regressão e classificação com dados simulados e reais, utilizando regressão logística e regressão linear. A partir das explicações geradas, o autor utilizou a métrica NMR para avaliar o impacto da colinearidade, concluindo que o AEC é mais robusto e estável do que o SHAP tradicional.

Por fim, \citeonline{Salih2024MiniReview} realizaram uma revisão sistemática da literatura para investigar como as técnicas de Inteligência Artificial Explicável (XAI) lidam com a multicolinearidade entre variáveis. Após filtrar artigos das bases Web of Science, Scopus e IEEE Xplore, identificaram apenas sete abordagens que tratam explicitamente esse problema. Os autores destacam que não existe, até o momento, uma técnica de XAI que por natureza mitigue o impacto da dependência entre atributos. Além disso, observam que as soluções existentes são limitadas: ou adaptadas para métodos específicos (como o SHAP), ou restritas a explicações locais, concluindo que são necessários avanços metodológicos que considerem interações complexas entre atributos correlacionados, tanto na geração quanto na visualização das explicações.



%\section{Considerações Finais}\label{sec:fund_consideracoes_finais}
%\lipsum[35]

