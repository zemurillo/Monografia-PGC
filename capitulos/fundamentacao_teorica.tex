% ----------------------------------------------------------
% Introdução 
% Capítulo sem numeração, mas presente no Sumário
% ----------------------------------------------------------

\chapter[Fundamentação Teórica]{Fundamentação Teórica}

\section{Colinearidade}\label{sec:colineariedade}

Colinearidade é um fenômeno que ocorre quando duas ou mais variáveis preditoras de um modelo estatístico apresentam uma relação linear significativa \cite{Carsten2012Collinearity}, isto é, quando estão altamente correlacionadas \cite{Das2019Analysis,Vatcheva2016Multicollinearity,Streukens2023Multicollinearity}, de modo que uma variável pode ser aproximadamente expressa como combinação linear das demais. No caso de colinearidade exata, essa relação é perfeita, ou seja, uma variável é completamente determinada pelas outras.

Formalmente, considera-se que existe colinearidade entre duas variáveis $X_i$ e $X_j$ quando existem coeficientes $\alpha$ e $\beta$ tais que:
\begin{equation}
	X_j \approx \alpha X_i + \beta,
\end{equation}
enquanto, na colinearidade perfeita, a relação é exata:
\begin{equation}
	X_j = \alpha X_i + \beta.
\end{equation}

Esse fenômeno reduz a independência entre as variáveis preditoras, dificultando a identificação de seus efeitos individuais, e pode afetar tanto modelos estatísticos clássicos, como Regressão Linear e Regressão Logística, quanto técnicas modernas de Inteligência Artificial Explicável (XAI) \cite{salih2025a_perpective}.

Segundo \citeonline{Carsten2012Collinearity} e \citeonline{weisberg2013applied}, colinearidade e multicolinearidade podem ser tratados como sinônimos. Já \citeonline{kim2019multicollinearity} distingue os termos: colinearidade refere-se ao caso em que uma variável apresenta forte relação linear com outra, enquanto multicolinearidade ocorre quando uma variável possui forte relação linear com duas ou mais variáveis. Neste trabalho, os termos serão utilizados de forma equivalente, privilegiando a denominação colinearidade.

Um indicador simples e frequentemente utilizado para detectar colinearidade é o coeficiente de correlação de Pearson entre pares de variáveis preditoras. O coeficiente de correlação de Pearson ($r$) mede o grau de associação linear entre duas variáveis $X$ e $Y$, sendo definido como:
\begin{equation}
	r = \frac{\text{Cov}(X,Y)}{\sigma_X \, \sigma_Y},
\end{equation}
onde $\text{Cov}(X,Y)$ é a covariância entre $X$ e $Y$, dada por:
\begin{equation}
	\text{Cov}(X,Y) = \frac{1}{n-1} \sum_{k=1}^{n} (X_k - \bar{X})(Y_k - \bar{Y}),
\end{equation}
e $\sigma_X$ e $\sigma_Y$ representam os respectivos desvios-padrão das variáveis $X$ e $Y$.

O valor de $r$ varia no intervalo $[-1,1]$: valores próximos de $1$ indicam forte correlação linear positiva; valores próximos de $-1$ indicam forte correlação linear negativa; e valores próximos de $0$ indicam ausência de dependência linear \cite{PearsonCorrelationCoefficient}. Quanto maior o valor absoluto de $r$, maior a dependência linear entre as variáveis, sugerindo a presença de colinearidade. Em particular, quando $|r| = 1$, há colinearidade perfeita, enquanto $|r| = 0$ indica ausência de qualquer relação linear \cite{kim2019multicollinearity}. Como regra prática, \citeonline{Carsten2012Collinearity} recomenda considerar $|r| \geq 0{,}7$ como um limiar a partir do qual a colinearidade pode distorcer significativamente as estimativas dos modelos. Por outro lado, \citeonline{Streukens2023Multicollinearity} sugere que valores a partir de $|r| \geq 0{,}3$ já podem indicar sinais relevantes de colinearidade. Complementarmente, \citeonline{Vatcheva2016Multicollinearity} utilizou diferentes graus de correlação de Pearson para caracterizar a intensidade da colinearidade: valores fracos (\(0 \leq |r| < 0,3\)), moderados (\(0,3 \leq |r| < 0,7\)) e fortes (\(|r| \geq 0,7\)).

Na prática, algum grau de colinearidade entre variáveis preditoras é praticamente inevitável, podendo surgir por diferentes motivos \cite{Carsten2012Collinearity}:

\begin{itemize}
	\item \textbf{Colinearidade intrínseca:} ocorre quando variáveis colineares são diferentes manifestações de um mesmo processo subjacente, muitas vezes não mensurável (variável latente). Por exemplo, peso e altura de uma pessoa estão fortemente relacionados, pois ambos refletem o tamanho corporal.
	
	\item \textbf{Colinearidade composicional:} surge em dados em que as variáveis representam partes de um todo e, portanto, não são independentes entre si. Por exemplo, se medirmos a proporção de três tipos de frutas em uma cesta (maçãs, laranjas e bananas) que somam sempre 100\%, um aumento na proporção de maçãs implica necessariamente a diminuição da proporção de laranjas ou bananas.
	
	\item \textbf{Colinearidade incidental:} ocorre quando variáveis preditoras parecem colineares apenas por acaso, como em amostras pequenas, nas quais nem todas as combinações de condições possíveis estão representadas.
\end{itemize}





%\section{Inteligência Artificial Explicável}\label{sec:xai}
%\lipsum[35]

%\subsection{SHAP}
%\lipsum[35]

%\subsection{LIME}
%\lipsum[35]

%\section{Algoritmos de Classificação}\label{sec:alg_class}
%\lipsum[35]
%\section {Problemas de classificação}
%\subsection{Algoritmos utilizados em problemas de classificação}\label{sec:algoritmos_classificacao}

%\section{Inteligência Artificial Explicável (XAI)}

%\subsection{Algoritmos de XAI}\label{sec:algoritmos_xai}

\section{Trabalhos relacionados}\label{sec:trabalhos_relacionados}

Alguns trabalhos foram desenvolvidos com o objetivo de compreender e mitigar os efeitos da colinearidade nos algoritmos de Inteligência Artificial Explicável (XAI). \citeonline{Basu2022Multicollinearity} apresentaram um framework matemático para corrigir a multicolinearidade no cálculo de valores de Shapley. Eles argumentam que a versão tradicional do SHAP assume independência entre atributos, o que não é realista e pode distorcer a atribuição de importância. Para resolver isso, propuseram um ajuste matricial que corrige os valores de Shapley considerando as correlações entre as variáveis, de modo que a importância individual se torne independente da multicolinearidade. Além disso, estenderam essa correção para calcular efeitos combinados de pares (ou grupos) de variáveis correlacionadas, somando os valores ajustados. O método foi validado em problemas reais de classificação, demonstrando boa eficiência computacional e explicações mais consistentes em presença de variáveis correlacionadas.

Dando continuidade à discussão sobre os impactos da multicolinearidade na explicabilidade, \citeonline{Salih2022Investigating} propuseram um novo critério de estabilidade para avaliar técnicas de XAI aplicadas à classificação de demência com base em imagens de ressonância magnética. Os autores treinaram modelos de classificação para diferenciar pacientes com e sem demência e analisaram a robustez dos rankings de importância gerados por métodos explicativos. A principal contribuição foi o desenvolvimento do NMR (\textit{Normalized Movement Rate}), um critério que quantifica a estabilidade das explicações fornecidas por técnicas de XAI, especialmente em cenários com multicolinearidade entre atributos. Os resultados demonstraram que o NMR melhora a confiabilidade na identificação de variáveis informativas, contribuindo para uma personalização mais segura do monitoramento clínico.

A partir da mesma motivação, \citeonline{Salih2024Characterizing} apresentaram o método \textit{Modified Index Position} (MIP) como uma solução simples e agnóstica ao modelo para ajustar os rankings de importância de variáveis gerados por métodos de XAI, como o SHAP, especialmente em contextos com colinearidade entre atributos. A abordagem consiste em remover iterativamente a variável mais importante apontada pela técnica de XAI, retreinar o modelo e reaplicar a explicabilidade, observando como as demais variáveis mudam de posição no ranking. Isso permite reordenar a importância original de forma a refletir dependências entre as variáveis. Aplicado a uma tarefa de classificação de gênero (homem ou mulher) com base em nove fenótipos cardíacos, o método demonstrou rankings mais robustos e menos sensíveis à colinearidade em comparação ao SHAP tradicional, sendo validado por análise de componentes principais e plausibilidade biológica.

Complementando esse estudo, \citeonline{salih2025a_perpective} investigaram as limitações dos métodos SHAP e LIME ao aplicá-los separadamente em dois conjuntos de dados distintos. Em cada experimento, treinaram quatro modelos de classificação (LightGBM, Regressão Logística, Árvore de Decisão e SVC) e analisaram como cada variável era explicada por cada modelo. Os resultados mostraram que ambos os métodos apresentaram forte dependência do modelo, com variações na ordem e na direção da contribuição das variáveis em cada classificador. Além disso, a presença de colinearidade comprometeu a interpretação, já que variáveis correlacionadas recebiam baixa importância por serem explicadas por outras. Para mitigar esses efeitos, os autores propuseram o uso da métrica NMR para avaliar a estabilidade das explicações entre os modelos e o método MIP para ajustar a importância das variáveis considerando a multicolinearidade. 

Explorando outra abordagem para lidar com esse desafio, \citeonline{Salih2025AdditiveEffects} propôs o método \textit{Additive Effects of Collinearity (AEC)} para superar as limitações de métodos de XAI em contextos com colinearidade entre variáveis. O autor argumenta que técnicas como o SHAP e LIME assumem independência entre os atributos, o que pode distorcer os rankings de importância. O AEC contorna esse problema ao decompor modelos multivariáveis em modelos univariáveis, estimando o efeito isolado de cada variável e, depois, somando esses efeitos considerando suas interdependências. O método foi validado em tarefas de regressão e classificação com dados simulados e reais, utilizando regressão logística e regressão linear. A partir das explicações geradas, o autor utilizou a métrica NMR para avaliar o impacto da colinearidade, concluindo que o AEC é mais robusto e estável do que o SHAP tradicional.

Por fim, \citeonline{Salih2024MiniReview} realizou uma revisão sistemática da literatura para investigar como as técnicas de Inteligência Artificial Explicável (XAI) lidam com a multicolinearidade entre variáveis. Após filtrar artigos das bases Web of Science, Scopus e IEEE Xplore, foram identificadas apenas sete abordagens que tratam explicitamente esse problema. Os autores destacam que não existe, até o momento, uma técnica de XAI que por natureza mitigue o impacto da dependência entre atributos. Além disso, observam que as soluções existentes são limitadas: ou adaptadas para métodos específicos (como o SHAP), ou restritas a explicações locais, concluindo que são necessários avanços metodológicos que considerem interações complexas entre atributos correlacionados, tanto na geração quanto na visualização das explicações.



%\section{Considerações Finais}\label{sec:fund_consideracoes_finais}
%\lipsum[35]

